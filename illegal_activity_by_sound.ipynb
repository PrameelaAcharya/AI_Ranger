{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49107bef-3cc1-4147-94e8-d9dfa7bb4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "# load PANNs-like model (torchvggish is trained on AudioSet)\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"harritaylor/torchvggish\")\n",
    "model = AutoModel.from_pretrained(\"harritaylor/torchvggish\")\n",
    "\n",
    "# load an audio file (wav, 16kHz mono)\n",
    "y, sr = librosa.load(\"forest_sample.wav\", sr=16000, mono=True)\n",
    "\n",
    "inputs = extractor(y, sampling_rate=sr, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(outputs.last_hidden_state.shape)  # embedding vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24d7ef-d757-48e0-a541-94956d7d357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install --upgrade numpy\n",
    "!pip install --upgrade pandas scipy torch librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b5d09-9955-449c-91bd-c017f2f207fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall numpy pandas scipy torch librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34154c3-ab37-491b-99e1-e654e8050187",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall numpy==1.26.4 pandas==2.2.2 scipy==1.13.1 librosa==0.10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05cf19-e72f-48af-87ac-a8c7d9a99c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch, librosa\n",
    "\n",
    "# Load feature extractor + model\n",
    "processor = AutoProcessor.from_pretrained(\"qiuqiangkong/panns\")\n",
    "model = AutoModel.from_pretrained(\"qiuqiangkong/panns\")\n",
    "\n",
    "# Example audio\n",
    "waveform, sr = librosa.load(\"forest_sound.wav\", sr=32000, mono=True)\n",
    "inputs = processor(waveform, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs).last_hidden_state.mean(dim=1)  # pooled embedding\n",
    "\n",
    "print(\"Embedding shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23d2f5c-8b57-4ddf-9bc6-419c2a548d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install librosa==0.10.1 soundfile\n",
    "!pip install h5py pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e7758-98c4-4557-8c27-879059939219",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p pretrained\n",
    "!wget -O pretrained/Cnn14.pth https://zenodo.org/record/3987831/files/Cnn14.pth?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19fe4e8-aef8-49b8-ad6a-92cf0114f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "\n",
    "os.makedirs(\"pretrained\", exist_ok=True)\n",
    "url = \"https://zenodo.org/record/3987831/files/Cnn14.pth?download=1\"\n",
    "r = requests.get(url, stream=True)\n",
    "\n",
    "with open(\"pretrained/Cnn14.pth\", \"wb\") as f:\n",
    "    for chunk in r.iter_content(chunk_size=8192):\n",
    "        if chunk:\n",
    "            f.write(chunk)\n",
    "\n",
    "print(\"âœ… Download complete: pretrained/Cnn14.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc089ad-45b5-4cdb-9b82-0bfee979d755",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x0a'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m Cnn14(sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32000\u001b[39m, window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, hop_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m320\u001b[39m,\n\u001b[1;32m     14\u001b[0m               mel_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, fmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, fmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14000\u001b[39m, classes_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m527\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load checkpoint safely\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/serialization.py:1549\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1547\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(\n\u001b[1;32m   1550\u001b[0m     opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args\n\u001b[1;32m   1551\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/serialization.py:1797\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1794\u001b[0m         \u001b[38;5;66;03m# if not a tarfile, reset file offset and proceed\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m         f\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1797\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[1;32m   1799\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x0a'."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "# Add repo path (adjust if needed)\n",
    "sys.path.append(\"audioset_tagging_cnn/pytorch\")\n",
    "\n",
    "from models import Cnn14\n",
    "\n",
    "# Path to your model\n",
    "checkpoint_path = \"/Users/prasad/pretrained/Cnn14.pth\"\n",
    "\n",
    "# Initialize model\n",
    "model = Cnn14(sample_rate=32000, window_size=1024, hop_size=320,\n",
    "              mel_bins=64, fmin=50, fmax=14000, classes_num=527)\n",
    "\n",
    "# Load checkpoint safely\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=False)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced9e2c7-c363-4fd8-a7e5-3bc5dc28c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"audioset_tagging_cnn/pytorch\")\n",
    "\n",
    "from models import Cnn14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d882dd2-2900-417a-acd3-25f0802caf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchlibrosa in /opt/anaconda3/lib/python3.12/site-packages (0.1.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchlibrosa) (1.26.4)\n",
      "Requirement already satisfied: librosa>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchlibrosa) (0.10.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.8.0->torchlibrosa) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.8.0->torchlibrosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.8.0->torchlibrosa) (1.7.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.8.0->torchlibrosa) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.8.0->torchlibrosa) (5.2.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.8.0->torchlibrosa) (0.61.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.8.0->torchlibrosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.8.0->torchlibrosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.8.0->torchlibrosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.8.0->torchlibrosa) (4.15.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.8.0->torchlibrosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from librosa>=0.8.0->torchlibrosa) (1.1.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from lazy-loader>=0.1->librosa>=0.8.0->torchlibrosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/anaconda3/lib/python3.12/site-packages (from numba>=0.51.0->librosa>=0.8.0->torchlibrosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from pooch>=1.0->librosa>=0.8.0->torchlibrosa) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from pooch>=1.0->librosa>=0.8.0->torchlibrosa) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.8.0->torchlibrosa) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.8.0->torchlibrosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.8.0->torchlibrosa) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.8.0->torchlibrosa) (2025.8.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.20.0->librosa>=0.8.0->torchlibrosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from soundfile>=0.12.1->librosa>=0.8.0->torchlibrosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.8.0->torchlibrosa) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchlibrosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de5ad9d-afcd-4971-a8c6-3a470b45a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new one \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43547da0-e1f9-4b91-bd75-685d2d10d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"audioset_tagging_cnn/pytorch\")\n",
    "\n",
    "from models import Cnn14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7e775f4-aa38-45b9-ac83-07ebb772803e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   251  100   251    0     0    188      0  0:00:01  0:00:01 --:--:--   188\n",
      "100 14206  100 14206    0     0   5498      0  0:00:02  0:00:02 --:--:-- 15193\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /Users/prasad/pretrained\n",
    "!curl -L -o /Users/prasad/pretrained/Cnn14.pth https://zenodo.org/record/3987831/files/Cnn14.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab1a7da-750e-4dc4-bf58-b848b333b67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 prasad  staff   312M  1 Sep 10:55 /Users/prasad/Documents/Major_Project/Cnn14.pth\n"
     ]
    }
   ],
   "source": [
    "ls -lh /Users/prasad/Documents/Major_Project/Cnn14.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbeecc7e-09ad-49df-87b8-b53247dac160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully!\n",
      "âœ… Inference done!\n",
      "Predictions shape: (527,)\n",
      "\n",
      "ðŸŽ¯ Top Predictions:\n",
      "137: Music (0.167)\n",
      "411: Sewing machine (0.099)\n",
      "506: Inside, small room (0.085)\n",
      "343: Engine (0.080)\n",
      "513: Noise (0.072)\n",
      "300: Vehicle (0.052)\n",
      "138: Musical instrument (0.040)\n",
      "0: Speech (0.035)\n",
      "523: Vibration (0.031)\n",
      "348: Medium engine (mid frequency) (0.024)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add audioset repo to path (adjust if your repo is in another folder)\n",
    "sys.path.append(\"audioset_tagging_cnn\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"audioset_tagging_cnn\")  # path to the cloned repo\n",
    "\n",
    "from pytorch.pytorch_utils import move_data_to_device\n",
    "from pytorch.models import Cnn14\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load pretrained model\n",
    "# -----------------------------\n",
    "checkpoint_path = \"/Users/prasad/Documents/Major_Project/Cnn14.pth\"  # change if needed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Cnn14(sample_rate=32000, window_size=1024, hop_size=320,\n",
    "              mel_bins=64, fmin=50, fmax=14000, classes_num=527)\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Model loaded successfully!\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load an audio file\n",
    "# -----------------------------\n",
    "wav_path = \"/Users/prasad/Documents/Major_Project/machine_sound.wav\"  # replace with your file\n",
    "waveform, sr = torchaudio.load(wav_path)\n",
    "\n",
    "#import torchaudio\n",
    "#wav_path = \"video.mp4\"  # your mp4 file\n",
    "#waveform, sr = torchaudio.load(wav_path)  # works if ffmpeg backend is enabled\n",
    "\n",
    "# Resample if not 32kHz\n",
    "if sr != 32000:\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=32000)(waveform)\n",
    "    sr = 32000\n",
    "\n",
    "# Mono\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Get model prediction\n",
    "# -----------------------------\n",
    "with torch.no_grad():\n",
    "    waveform = move_data_to_device(waveform, device)\n",
    "    output_dict = model(waveform, None)\n",
    "\n",
    "    # Clip-level probabilities (527 AudioSet classes)\n",
    "    clipwise_output = output_dict[\"clipwise_output\"].cpu().numpy()[0]\n",
    "\n",
    "print(\"âœ… Inference done!\")\n",
    "print(\"Predictions shape:\", clipwise_output.shape)  # should be (527,)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Map class IDs -> labels\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Use the class_labels_indices.csv file inside the repo\n",
    "class_map_path = \"audioset_tagging_cnn/metadata/class_labels_indices.csv\"\n",
    "df = pd.read_csv(class_map_path)\n",
    "\n",
    "# Build index â†’ label mapping\n",
    "class_map = {int(row[\"index\"]): row[\"display_name\"] for _, row in df.iterrows()}\n",
    "\n",
    "\n",
    "# Print top-10 predictions\n",
    "topk = 10\n",
    "top_indices = clipwise_output.argsort()[-topk:][::-1]\n",
    "\n",
    "print(\"\\nðŸŽ¯ Top Predictions:\")\n",
    "for idx in top_indices:\n",
    "    print(f\"{idx}: {class_map[idx]} ({clipwise_output[idx]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6ffd808-2308-47b8-86be-28a96c2b8200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-01 11:23:21--  https://raw.githubusercontent.com/qiuqiangkong/audioset_tagging_cnn/master/utils/class_labels_indices.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2025-09-01 11:23:21 ERROR 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/qiuqiangkong/audioset_tagging_cnn/master/utils/class_labels_indices.csv -O class_labels_indices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b80acf7-a46f-47bf-bce8-a686b7b640dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    14  100    14    0     0     40      0 --:--:-- --:--:-- --:--:--    40\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o class_labels_indices.csv \"https://raw.githubusercontent.com/qiuqiangkong/audioset_tagging_cnn/master/utils/class_labels_indices.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36d8a66e-b60c-4b1e-8432-f8460162c7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 prasad  staff    14B  1 Sep 11:27 class_labels_indices.csv\n"
     ]
    }
   ],
   "source": [
    "ls -lh class_labels_indices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7f984-0581-478a-b58f-390eec01d5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a1ffe-e1b4-4ff2-bacc-2fb304636195",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                            Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a641027-bb7a-4286-adc9-5a1ed28d924c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f79e1a4-7c8f-47e6-849b-15d5b4a0312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LEGAL / No illegal sound detected.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: clipwise_output is your prediction vector of shape (527,)\n",
    "# It should be float32/float64 values between 0â€“1\n",
    "clipwise_output = output_dict[\"clipwise_output\"].cpu().numpy()[0].astype(float)\n",
    "\n",
    "# Define which classes are ILLEGAL (IDs from ontology.csv)\n",
    "illegal_class_ids = {\n",
    "    427: \"Gunshot, gunfire\",\n",
    "    428: \"Machine gun\",\n",
    "    431: \"Cap gun\",\n",
    "    377: \"Chainsaw\",\n",
    "    370: \"Explosion\",\n",
    "    415: \"Rifle\",\n",
    "    416: \"Pistol\",\n",
    "    # add more if you want (e.g. \"Bomb\", \"Siren\" if used for poaching alerts)\n",
    "}\n",
    "\n",
    "# Confidence threshold\n",
    "THRESHOLD = 0.2  \n",
    "\n",
    "# Collect illegal predictions\n",
    "illegal_preds = []\n",
    "for idx, name in illegal_class_ids.items():\n",
    "    score = float(clipwise_output[idx])   # ensure float\n",
    "    if score > THRESHOLD:\n",
    "        illegal_preds.append((name, score))\n",
    "\n",
    "# Final classification\n",
    "if illegal_preds:\n",
    "    print(\"ðŸš¨ ILLEGAL activity detected!\")\n",
    "    for name, score in illegal_preds:\n",
    "        print(f\"   - {name}: {score:.3f}\")\n",
    "else:\n",
    "    print(\"âœ… LEGAL / No illegal sound detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc1debe1-e8ea-4e8a-8aaf-9263bb51f39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤ Listening for illegal activities...\n",
      "Detected: Vehicle (0.14)\n",
      "Detected: Vehicle (0.45)\n",
      "Detected: Vehicle (0.58)\n",
      "Detected: Vehicle (0.46)\n",
      "Detected: Vehicle (0.32)\n",
      "Detected: Vehicle (0.52)\n",
      "Detected: Vehicle (0.58)\n",
      "Detected: Machine gun (0.77)\n",
      "ðŸš¨ ALERT: Illegal activity detected! ðŸš¨\n",
      "Detected: Speech (0.81)\n",
      "Detected: Jackhammer (0.71)\n",
      "Detected: Machine gun (0.24)\n",
      "ðŸš¨ ALERT: Illegal activity detected! ðŸš¨\n",
      "Detected: Machine gun (0.44)\n",
      "ðŸš¨ ALERT: Illegal activity detected! ðŸš¨\n",
      "Detected: Rub (0.11)\n",
      "Detected: Typing (0.32)\n",
      "Detected: Tick (0.14)\n",
      "Detected: Typing (0.67)\n",
      "Detected: Typing (0.38)\n",
      "Detected: Machine gun (0.28)\n",
      "ðŸš¨ ALERT: Illegal activity detected! ðŸš¨\n",
      "Detected: Typing (0.20)\n",
      "Detected: Vehicle (0.37)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸš¨ ALERT: Illegal activity detected! ðŸš¨\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# TODO: Send ROS message or trigger actuator here\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import pyaudio\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(\"audioset_tagging_cnn\")  # path to the cloned repo\n",
    "\n",
    "from pytorch.pytorch_utils import move_data_to_device\n",
    "from pytorch.models import Cnn14\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "ILLEGAL_CLASSES = [\"Gunshot, gunfire\", \"Machine gun\", \"Chainsaw\", \"Explosion\", \"Cap gun\",\"Machine gun\",\"Mechanisms\"]\n",
    "THRESHOLD = 0.2  # probability cutoff\n",
    "\n",
    "# Load pretrained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Cnn14(sample_rate=32000, window_size=1024, hop_size=320,\n",
    "              mel_bins=64, fmin=50, fmax=14000, classes_num=527)\n",
    "\n",
    "checkpoint = torch.load(\"Documents/Major_Project/Cnn14.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load class labels\n",
    "import pandas as pd\n",
    "labels = pd.read_csv(\"audioset_tagging_cnn/metadata/class_labels_indices.csv\")\n",
    "id_to_label = {int(row[\"index\"]): row[\"display_name\"] for _, row in labels.iterrows()}\n",
    "\n",
    "# -----------------------\n",
    "# Microphone Setup\n",
    "# -----------------------\n",
    "CHUNK = 32000 * 2  # 2 sec of audio at 32kHz\n",
    "FORMAT = pyaudio.paFloat32\n",
    "CHANNELS = 1\n",
    "RATE = 32000\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"ðŸŽ¤ Listening for illegal activities...\")\n",
    "\n",
    "# -----------------------\n",
    "# Real-time loop\n",
    "# -----------------------\n",
    "while True:\n",
    "    data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "    audio = np.frombuffer(data, dtype=np.float32)\n",
    "    waveform = torch.tensor(audio).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(waveform, None)\n",
    "        clipwise_output = output[\"clipwise_output\"].cpu().numpy()[0]\n",
    "\n",
    "    # Top prediction\n",
    "    top_idx = int(np.argmax(clipwise_output))\n",
    "    top_label = id_to_label[top_idx]\n",
    "    top_prob = clipwise_output[top_idx]\n",
    "\n",
    "    print(f\"Detected: {top_label} ({top_prob:.2f})\")\n",
    "\n",
    "    # Check if illegal\n",
    "    if top_label in ILLEGAL_CLASSES and top_prob > THRESHOLD:\n",
    "        print(\"ðŸš¨ ALERT: Illegal activity detected! ðŸš¨\")\n",
    "        # TODO: Send ROS message or trigger actuator here\n",
    "\n",
    "    time.sleep(1)  # small delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7dd49b6-abcb-4be3-bb59-ffe5450ecee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updating Homebrew...\u001b[0m\n",
      "Adjust how often this is run with `$HOMEBREW_AUTO_UPDATE_SECS` or disable with\n",
      "`$HOMEBREW_NO_AUTO_UPDATE=1`. Hide these hints with `$HOMEBREW_NO_ENV_HINTS=1` (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
      "Updated 2 taps (homebrew/core and homebrew/cask).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Formulae\u001b[0m\n",
      "darker: Apply Black formatting only in regions changed since last commit\n",
      "docker-debug: Use new container attach on already container go on debug\n",
      "\n",
      "You have \u001b[1m26\u001b[0m outdated formulae installed.\n",
      "\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching downloads for: \u001b[32mportaudio\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/portaudio/manifests/19.7.0-1\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mportaudio\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/portaudio/blobs/sha256:8ad9f1c1\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring portaudio--19.7.0.arm64_sequoia.bottle.1.tar.gz\u001b[0m\n",
      "ðŸº  /opt/homebrew/Cellar/portaudio/19.7.0: 34 files, 545.9KB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup portaudio`...\u001b[0m\n",
      "Disable this behaviour by setting `HOMEBREW_NO_INSTALL_CLEANUP=1`.\n",
      "Hide these hints with `HOMEBREW_NO_ENV_HINTS=1` (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNo outdated dependents to upgrade!\u001b[0m\n",
      "Collecting pyaudio\n",
      "  Using cached PyAudio-0.2.14.tar.gz (47 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pyaudio\n",
      "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyaudio: filename=pyaudio-0.2.14-cp312-cp312-macosx_11_0_arm64.whl size=24157 sha256=dd7d18eaa0e85b06ea98652daef2f4de8ad3826d382a6a44feeafb5d72d9efed\n",
      "  Stored in directory: /Users/prasad/Library/Caches/pip/wheels/68/c7/33/c6a6b210cb5819ec5c219928c794a447742a7d86d21c0b92e6\n",
      "Successfully built pyaudio\n",
      "Installing collected packages: pyaudio\n",
      "Successfully installed pyaudio-0.2.14\n"
     ]
    }
   ],
   "source": [
    "!brew install portaudio\n",
    "!pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5a60d58-95b4-4dca-a45d-288986f58407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved to illegal_detector.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path where you want to save\n",
    "save_path = \"illegal_detector.pth\"\n",
    "\n",
    "# Save model state\n",
    "torch.save(model.state_dict(), save_path)\n",
    "\n",
    "print(f\"âœ… Model saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c2cd7-9af0-4638-993d-12e247b2400d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
